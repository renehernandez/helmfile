{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"CONTRIBUTING/","text":"DCO + License \u00b6 By contributing to helmfile , you accept and agree to the following DCO and license terms and conditions for your present and future Contributions submitted to the helmfile project. DCO License Developing helmfile \u00b6 Locate your GOPATH , usually ~/go , and run: $ go get github.com/roboll/helmfile $ cd $GOPATH/src/github.com/roboll/helmfile $ git checkout -b your-shiny-new-feature origin/master ... $ git commit -m 'feat: do whatever for whatever purpose This adds ... by: - Adding ... - Changing ... - Removing... Resolves #ISSUE_NUMBER ' $ hub fork $ git push YOUR_GITHUB_USER your-shiny-new-feature $ hub pull-request Note that the above tutorial uses hub just for ease of explanation. Please use whatever tool or way to author your pull request!","title":"Contributing"},{"location":"CONTRIBUTING/#dco-license","text":"By contributing to helmfile , you accept and agree to the following DCO and license terms and conditions for your present and future Contributions submitted to the helmfile project. DCO License","title":"DCO + License"},{"location":"CONTRIBUTING/#developing-helmfile","text":"Locate your GOPATH , usually ~/go , and run: $ go get github.com/roboll/helmfile $ cd $GOPATH/src/github.com/roboll/helmfile $ git checkout -b your-shiny-new-feature origin/master ... $ git commit -m 'feat: do whatever for whatever purpose This adds ... by: - Adding ... - Changing ... - Removing... Resolves #ISSUE_NUMBER ' $ hub fork $ git push YOUR_GITHUB_USER your-shiny-new-feature $ hub pull-request Note that the above tutorial uses hub just for ease of explanation. Please use whatever tool or way to author your pull request!","title":"Developing helmfile"},{"location":"LICENSE/","text":"MIT License Copyright (c) 2017 rob boll Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"PATHS/","text":"Paths Overivew \u00b6 Using manifest files in conjunction with command line argument can be a bit confusing. A few rules to clear up this ambiguity: Absolute paths are always resolved as absolute paths Relative paths referenced in the helmfile manifest itself are relative to that manifest Relative paths referenced on the command line are relative to the current working directory the user is in Examples \u00b6 There are several examples that we can go through in the /examples folder which demonstrate this. Local Execution This is an example of a Helmfile manifest referencing a local value directly. Indirect: helmfile -f examples/deployments/local/charts.yaml sync Direct: cd examples/deployments/local/ helmfile sync Relative Paths in Helmfile This is an example of a Helmfile manifest using relative paths for values. Indirect: helmfile -f examples/deployments/dev/charts.yaml sync Direct: cd examples/deployments/dev/ helmfile sync Relative Paths in Helmfile w/ \u2013values overrides This is an example of a Helmfile manifest using relative paths for values including an additional --values from the command line. NOTE: The --values is resolved relative to the CWD of the terminal not the Helmfile manifest. You can see this with the replicas being adjusted to 3 now for the deployment. Indirect: helmfile -f examples/deployments/dev/charts.yaml sync --values values/replica-values.yaml Direct: cd examples/deployments/dev/ helmfile sync --values ../../values/replica-values.yaml","title":"Paths Overview"},{"location":"PATHS/#paths-overivew","text":"Using manifest files in conjunction with command line argument can be a bit confusing. A few rules to clear up this ambiguity: Absolute paths are always resolved as absolute paths Relative paths referenced in the helmfile manifest itself are relative to that manifest Relative paths referenced on the command line are relative to the current working directory the user is in","title":"Paths Overivew"},{"location":"PATHS/#examples","text":"There are several examples that we can go through in the /examples folder which demonstrate this. Local Execution This is an example of a Helmfile manifest referencing a local value directly. Indirect: helmfile -f examples/deployments/local/charts.yaml sync Direct: cd examples/deployments/local/ helmfile sync Relative Paths in Helmfile This is an example of a Helmfile manifest using relative paths for values. Indirect: helmfile -f examples/deployments/dev/charts.yaml sync Direct: cd examples/deployments/dev/ helmfile sync Relative Paths in Helmfile w/ \u2013values overrides This is an example of a Helmfile manifest using relative paths for values including an additional --values from the command line. NOTE: The --values is resolved relative to the CWD of the terminal not the Helmfile manifest. You can see this with the replicas being adjusted to 3 now for the deployment. Indirect: helmfile -f examples/deployments/dev/charts.yaml sync --values values/replica-values.yaml Direct: cd examples/deployments/dev/ helmfile sync --values ../../values/replica-values.yaml","title":"Examples"},{"location":"USERS/","text":"Helmfile Users \u00b6 Who\u2019s actually using Helmfile? The following table is compiled from public information, survey response and PRs. It\u2019s likely this is only a small selection of actual users, but it\u2019s a start. If you are using Helmfile in your company or organization, we would like to invite you to create a PR to add your information to this file. Organization Workload More Info Location Added reddit.com production GitHub issue , Talk San Francisco, CA April 2018 ricardo.ch production We\u2019re deploying our complete application platform using Helmfile. Zug, Switzerland April 2018 Hellofresh production We\u2019ve been using helmfile in production since April 2018 for deploying all our infrastructure applications Berlin, Germany April 2018 Cherre production We have no public posts about using Helmfile, but we have been using it for a long time now New York, NY October 2018 Sight Machine production We don\u2019t have anything publicly posted about it, but have been using it for quite a while in production. San Francisco, CA and Ann Arbor, MI December 2018 TailorMed production We\u2019re deploying all of our platform and all K8s resources using Helmfile. The power of Helmfile are more then amazing. Thank you! Tel-Aviv, Israel September 2020 VSHN \u2013 The DevOps Company production Zurich, Switzerland March 2019 Vlocity proof-of-concept Melbourne, Australia March 2019 transit production Blog post . Montreal, Canada March 2019 uniqkey production Wiki Page Copenhagen, Denmark April 2019 bitsofinfo production Used with helmfile-deploy to manage releases for dozens of apps USA July 2019 kloeckner-i production We are deploying our standard tools via helmfile to all our clusters. Berlin, Germany September 2019 American Express proof-of-concept Orchestration of both internal cluster workloads and local developer environments. London, GB January 2020 Sportradar production Since mid-2019, we\u2019ve been deploying our core infrastructure and several application stacks with Helmfile. St. Gallen, Switzerland March 2020 PedidosYa production Montevideo, Uruguay June 2020 Jenkins OSS production jenkins-infra/charts * July 2020 SettleMint production The SettleMint platform allows enterprises to spin up k8s clusters and deploy production grade blockchain networks and additional services. Helmfile is in charge of deploying these networks and services on demand out of the self service management ui. Belgium, Singapore, UAE, India October 2020 AutoTrader (UK) production We\u2019ve used Helmfile for 2+ years to deploy 400+ services UK October 2020 Trend Micro production We manage 9 k8s clusters in a mono git repository with Helmfile Taipei, Taiwan December 2019 William Hill production We have standardised on Helmfile for managing both our application services and our Kubernetes platform components. International March 2021 The Hyve production We\u2019ve been using Helmfile since 2019 to deploy Radar-base applications . Blog Post Utrecht, Netherlands April 2021","title":"Users"},{"location":"USERS/#helmfile-users","text":"Who\u2019s actually using Helmfile? The following table is compiled from public information, survey response and PRs. It\u2019s likely this is only a small selection of actual users, but it\u2019s a start. If you are using Helmfile in your company or organization, we would like to invite you to create a PR to add your information to this file. Organization Workload More Info Location Added reddit.com production GitHub issue , Talk San Francisco, CA April 2018 ricardo.ch production We\u2019re deploying our complete application platform using Helmfile. Zug, Switzerland April 2018 Hellofresh production We\u2019ve been using helmfile in production since April 2018 for deploying all our infrastructure applications Berlin, Germany April 2018 Cherre production We have no public posts about using Helmfile, but we have been using it for a long time now New York, NY October 2018 Sight Machine production We don\u2019t have anything publicly posted about it, but have been using it for quite a while in production. San Francisco, CA and Ann Arbor, MI December 2018 TailorMed production We\u2019re deploying all of our platform and all K8s resources using Helmfile. The power of Helmfile are more then amazing. Thank you! Tel-Aviv, Israel September 2020 VSHN \u2013 The DevOps Company production Zurich, Switzerland March 2019 Vlocity proof-of-concept Melbourne, Australia March 2019 transit production Blog post . Montreal, Canada March 2019 uniqkey production Wiki Page Copenhagen, Denmark April 2019 bitsofinfo production Used with helmfile-deploy to manage releases for dozens of apps USA July 2019 kloeckner-i production We are deploying our standard tools via helmfile to all our clusters. Berlin, Germany September 2019 American Express proof-of-concept Orchestration of both internal cluster workloads and local developer environments. London, GB January 2020 Sportradar production Since mid-2019, we\u2019ve been deploying our core infrastructure and several application stacks with Helmfile. St. Gallen, Switzerland March 2020 PedidosYa production Montevideo, Uruguay June 2020 Jenkins OSS production jenkins-infra/charts * July 2020 SettleMint production The SettleMint platform allows enterprises to spin up k8s clusters and deploy production grade blockchain networks and additional services. Helmfile is in charge of deploying these networks and services on demand out of the self service management ui. Belgium, Singapore, UAE, India October 2020 AutoTrader (UK) production We\u2019ve used Helmfile for 2+ years to deploy 400+ services UK October 2020 Trend Micro production We manage 9 k8s clusters in a mono git repository with Helmfile Taipei, Taiwan December 2019 William Hill production We have standardised on Helmfile for managing both our application services and our Kubernetes platform components. International March 2021 The Hyve production We\u2019ve been using Helmfile since 2019 to deploy Radar-base applications . Blog Post Utrecht, Netherlands April 2021","title":"Helmfile Users"},{"location":"advanced-features/","text":"Advanced Features \u00b6 Import Configuration Parameters into Helmfile Deploy Kustomization with Helmfile Adhoc Kustomization of Helm Charts Import Configuration Parameters into Helmfile \u00b6 Helmfile integrates vals to import configuration parameters from following backends: AWS SSM Parameter Store AWS SecretsManager Vault SOPS See Vals \u201cSuported Backends\u201d for the full list of available backends. This feature was implemented in https://github.com/roboll/helmfile/pull/906. If you\u2019re curious how it\u2019s designed and how it works, please consult the pull request. Deploy Kustomizations with Helmfile \u00b6 You can deploy kustomize \u201ckustomization\u201ds with Helmfile. Most of Kustomize operations that is usually done with kustomize edit can be done declaratively via Helm values.yaml files. Under the hood, Helmfile transforms the kustomization into a local chart in a temporary directory so that it can be helm upgrade --install ed. The transformation is done by generating (1)a temporary kustomization from various options and (2)temporary chart from the temporary kustomization. An example pseudo code for the transformation logic can be written as: $ TMPCHART=/tmp/sometmpdir $ mkdir -p ${TMPCHART}/templates $ somehow_generate_chart_yaml ${TMPCHART}/Chart.yaml $ TMPKUSTOMIZATION=/tmp/sometmpdir2 $ somehow_generate_temp_kustomization_yaml ${TMPKUSTOMIZATION}/kustomization.yaml $ kustomize build ${TMPKUSTOMIZATION}/kustomization.yaml > ${TMPCHART}/templates/all.yaml Let\u2019s say you have a helmfile.yaml that looks like the below: releases: - name: myapp chart: mykustomization values: - values.yaml Helmfile firstly generates a temporary kustomization.yaml that looks like: bases: - $(ABS_PATH_TO_HELMFILE_YAML}/mykustomization Followed by the below steps: Running kustomize edit set image $IMAGE for every $IMAGE generated from your values.yaml Running kustomize edit set nameprefix $NAMEPREFIX with the nameprefix specified in your values.yaml Running kustomize edit set namesuffix $NAMESUFFIX with the namesuffix specified in your values.yaml Running kustomize edit set namespace $NS with the namespace specified in your values.yaml A values.yaml file for kustomization would look like the below: images: # kustomize edit set image mysql=eu.gcr.io/my-project/mysql@canary - name: mysql newName: eu.gcr.io/my-project/mysql newTag: canary # kustomize edit set image myapp=my-registry/my-app@sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 - name: myapp digest: sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 newName: my-registry/my-app # kustomize edit set nameprefix foo- namePrefix: foo- # kustomize edit set namesuffix -bar nameSuffix: -bar # kustomize edit set namespace myapp namespace: myapp At this point, Helmfile can generate a complete kustomization from the base kustomization you specified in releases[].chart of your helmfile.yaml and values.yaml , which can be included in the temporary chart. After all, Helmfile just installs the temporary chart like standard charts, which allows you to manage everything with Helmfile regardless of each app is declared using a Helm chart or a kustomization. Please also see test/advanced/helmfile.yaml for an example of kustomization support and more. Adhoc Kustomization of Helm charts \u00b6 With Helmfile\u2019s integration with Helmfile, not only deploying Kustomization as a Helm chart, you can kustomize charts before installation. Currently, Helmfile allows you to set the following fields for kustomizing the chart: releases[].strategicMergePatches releases[].jsonPatches releases[].transformers strategicMergePatches \u00b6 You can add/update any Kubernetes resource field rendered from a Helm chart by specifying releases[].strategicMergePatches : repositories: - name: incubator url: https://kubernetes-charts-incubator.storage.googleapis.com releases: - name: raw1 chart: incubator/raw values: - resources: - apiVersion: v1 kind: ConfigMap metadata: name: raw1 namespace: default data: foo: FOO strategicMergePatches: - apiVersion: v1 kind: ConfigMap metadata: name: raw1 namespace: default data: bar: BAR Running helmfile template on the above example results in a ConfigMap called raw whose data is: foo: FOO bar: BAR Please note that the second data field bar is coming from the strategic-merge patch defined in the above helmfile.yaml. There\u2019s also releases[].jsonPatches that works similarly to strategicMergePatches but has additional capability to remove fields. Please also see test/advanced/helmfile.yaml for an example of patching support and more. transformers \u00b6 You can set transformers to apply Kustomize\u2019s transformers . Each item can be a path to a YAML or go template file, or an embedded transformer declaration as a YAML hash. It\u2019s often used to add common labels and annotations to your resources. In the below example. we add common annotations and labels every resource rendered from the aws-load-balancer-controller chart: releases: - name: \"aws-load-balancer-controller\" namespace: \"kube-system\" forceNamespace: \"kube-system\" chart: \"center/aws/aws-load-balancer-controller\" transformers: - apiVersion: builtin kind: AnnotationsTransformer metadata: name: notImportantHere annotations: area: 51 greeting: take me to your leader fieldSpecs: - path: metadata/annotations create: true - apiVersion: builtin kind: LabelTransformer metadata: name: notImportantHere labels: foo: bar fieldSpecs: - path: metadata/labels create: true As explained earlier, transformers can be not only a list of embedded transformers, but also YAML or go template files, or a mix of those three kinds. transformers: # Embedded transformer - apiVersion: builtin kind: AnnotationsTransformer metadata: name: notImportantHere annotations: area: 51 greeting: take me to your leader fieldSpecs: - path: metadata/annotations create: true # YAML file - path/to/transformer.yaml # Go template # The same set of template parameters as release values files templates is available. - path/to/transformer.yaml.gotmpl Please see https://github.com/kubernetes-sigs/kustomize/blob/master/examples/configureBuiltinPlugin.md#configuring-the-builtin-plugins-instead for more information on how to declare transformers.","title":"Advanced Features"},{"location":"advanced-features/#advanced-features","text":"Import Configuration Parameters into Helmfile Deploy Kustomization with Helmfile Adhoc Kustomization of Helm Charts","title":"Advanced Features"},{"location":"advanced-features/#import-configuration-parameters-into-helmfile","text":"Helmfile integrates vals to import configuration parameters from following backends: AWS SSM Parameter Store AWS SecretsManager Vault SOPS See Vals \u201cSuported Backends\u201d for the full list of available backends. This feature was implemented in https://github.com/roboll/helmfile/pull/906. If you\u2019re curious how it\u2019s designed and how it works, please consult the pull request.","title":"Import Configuration Parameters into Helmfile"},{"location":"advanced-features/#deploy-kustomizations-with-helmfile","text":"You can deploy kustomize \u201ckustomization\u201ds with Helmfile. Most of Kustomize operations that is usually done with kustomize edit can be done declaratively via Helm values.yaml files. Under the hood, Helmfile transforms the kustomization into a local chart in a temporary directory so that it can be helm upgrade --install ed. The transformation is done by generating (1)a temporary kustomization from various options and (2)temporary chart from the temporary kustomization. An example pseudo code for the transformation logic can be written as: $ TMPCHART=/tmp/sometmpdir $ mkdir -p ${TMPCHART}/templates $ somehow_generate_chart_yaml ${TMPCHART}/Chart.yaml $ TMPKUSTOMIZATION=/tmp/sometmpdir2 $ somehow_generate_temp_kustomization_yaml ${TMPKUSTOMIZATION}/kustomization.yaml $ kustomize build ${TMPKUSTOMIZATION}/kustomization.yaml > ${TMPCHART}/templates/all.yaml Let\u2019s say you have a helmfile.yaml that looks like the below: releases: - name: myapp chart: mykustomization values: - values.yaml Helmfile firstly generates a temporary kustomization.yaml that looks like: bases: - $(ABS_PATH_TO_HELMFILE_YAML}/mykustomization Followed by the below steps: Running kustomize edit set image $IMAGE for every $IMAGE generated from your values.yaml Running kustomize edit set nameprefix $NAMEPREFIX with the nameprefix specified in your values.yaml Running kustomize edit set namesuffix $NAMESUFFIX with the namesuffix specified in your values.yaml Running kustomize edit set namespace $NS with the namespace specified in your values.yaml A values.yaml file for kustomization would look like the below: images: # kustomize edit set image mysql=eu.gcr.io/my-project/mysql@canary - name: mysql newName: eu.gcr.io/my-project/mysql newTag: canary # kustomize edit set image myapp=my-registry/my-app@sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 - name: myapp digest: sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 newName: my-registry/my-app # kustomize edit set nameprefix foo- namePrefix: foo- # kustomize edit set namesuffix -bar nameSuffix: -bar # kustomize edit set namespace myapp namespace: myapp At this point, Helmfile can generate a complete kustomization from the base kustomization you specified in releases[].chart of your helmfile.yaml and values.yaml , which can be included in the temporary chart. After all, Helmfile just installs the temporary chart like standard charts, which allows you to manage everything with Helmfile regardless of each app is declared using a Helm chart or a kustomization. Please also see test/advanced/helmfile.yaml for an example of kustomization support and more.","title":"Deploy Kustomizations with Helmfile"},{"location":"advanced-features/#adhoc-kustomization-of-helm-charts","text":"With Helmfile\u2019s integration with Helmfile, not only deploying Kustomization as a Helm chart, you can kustomize charts before installation. Currently, Helmfile allows you to set the following fields for kustomizing the chart: releases[].strategicMergePatches releases[].jsonPatches releases[].transformers","title":"Adhoc Kustomization of Helm charts"},{"location":"advanced-features/#strategicmergepatches","text":"You can add/update any Kubernetes resource field rendered from a Helm chart by specifying releases[].strategicMergePatches : repositories: - name: incubator url: https://kubernetes-charts-incubator.storage.googleapis.com releases: - name: raw1 chart: incubator/raw values: - resources: - apiVersion: v1 kind: ConfigMap metadata: name: raw1 namespace: default data: foo: FOO strategicMergePatches: - apiVersion: v1 kind: ConfigMap metadata: name: raw1 namespace: default data: bar: BAR Running helmfile template on the above example results in a ConfigMap called raw whose data is: foo: FOO bar: BAR Please note that the second data field bar is coming from the strategic-merge patch defined in the above helmfile.yaml. There\u2019s also releases[].jsonPatches that works similarly to strategicMergePatches but has additional capability to remove fields. Please also see test/advanced/helmfile.yaml for an example of patching support and more.","title":"strategicMergePatches"},{"location":"advanced-features/#transformers","text":"You can set transformers to apply Kustomize\u2019s transformers . Each item can be a path to a YAML or go template file, or an embedded transformer declaration as a YAML hash. It\u2019s often used to add common labels and annotations to your resources. In the below example. we add common annotations and labels every resource rendered from the aws-load-balancer-controller chart: releases: - name: \"aws-load-balancer-controller\" namespace: \"kube-system\" forceNamespace: \"kube-system\" chart: \"center/aws/aws-load-balancer-controller\" transformers: - apiVersion: builtin kind: AnnotationsTransformer metadata: name: notImportantHere annotations: area: 51 greeting: take me to your leader fieldSpecs: - path: metadata/annotations create: true - apiVersion: builtin kind: LabelTransformer metadata: name: notImportantHere labels: foo: bar fieldSpecs: - path: metadata/labels create: true As explained earlier, transformers can be not only a list of embedded transformers, but also YAML or go template files, or a mix of those three kinds. transformers: # Embedded transformer - apiVersion: builtin kind: AnnotationsTransformer metadata: name: notImportantHere annotations: area: 51 greeting: take me to your leader fieldSpecs: - path: metadata/annotations create: true # YAML file - path/to/transformer.yaml # Go template # The same set of template parameters as release values files templates is available. - path/to/transformer.yaml.gotmpl Please see https://github.com/kubernetes-sigs/kustomize/blob/master/examples/configureBuiltinPlugin.md#configuring-the-builtin-plugins-instead for more information on how to declare transformers.","title":"transformers"},{"location":"remote-secrets/","text":"Secrets \u00b6 helmfile can handle secrets using helm-secrets plugin or using remote secrets storage (everything that package vals can handle vault, AWS SSM etc) This section will describe the second use case. Remote secrets \u00b6 This paragraph will describe how to use remote secrets storage (vault, SSM etc) in helmfile Fetching single key \u00b6 To fetch single key from remote secret storage you can use fetchSecretValue template function example below # helmfile.yaml repositories: - name: stable url: https://kubernetes-charts.storage.googleapis.com environments: default: values: - service: password: ref+vault://svc/#pass login: ref+vault://svc/#login releases: - name: service namespace: default labels: cluster: services secrets: vault chart: stable/svc version: 0.1.0 values: - service: login: {{ .Values.service.login | fetchSecretValue }} # this will resolve ref+vault://svc/#pass and fetch secret from vault password: {{ .Values.service.password | fetchSecretValue | quote }} # - values/service.yaml.gotmpl # alternatively Fetching multiple keys \u00b6 Alternatively you can use expandSecretRefs to fetch a map of secrets # values/service.yaml.gotmpl service: {{ .Values.service | expandSecretRefs | toYaml | nindent 2 }} This will produce # values/service.yaml service: login: svc-login # fetched from vault password: pass","title":"Secrets"},{"location":"remote-secrets/#secrets","text":"helmfile can handle secrets using helm-secrets plugin or using remote secrets storage (everything that package vals can handle vault, AWS SSM etc) This section will describe the second use case.","title":"Secrets"},{"location":"remote-secrets/#remote-secrets","text":"This paragraph will describe how to use remote secrets storage (vault, SSM etc) in helmfile","title":"Remote secrets"},{"location":"remote-secrets/#fetching-single-key","text":"To fetch single key from remote secret storage you can use fetchSecretValue template function example below # helmfile.yaml repositories: - name: stable url: https://kubernetes-charts.storage.googleapis.com environments: default: values: - service: password: ref+vault://svc/#pass login: ref+vault://svc/#login releases: - name: service namespace: default labels: cluster: services secrets: vault chart: stable/svc version: 0.1.0 values: - service: login: {{ .Values.service.login | fetchSecretValue }} # this will resolve ref+vault://svc/#pass and fetch secret from vault password: {{ .Values.service.password | fetchSecretValue | quote }} # - values/service.yaml.gotmpl # alternatively","title":"Fetching single key"},{"location":"remote-secrets/#fetching-multiple-keys","text":"Alternatively you can use expandSecretRefs to fetch a map of secrets # values/service.yaml.gotmpl service: {{ .Values.service | expandSecretRefs | toYaml | nindent 2 }} This will produce # values/service.yaml service: login: svc-login # fetched from vault password: pass","title":"Fetching multiple keys"},{"location":"shared-configuration-across-teams/","text":"Shared Configuration Across Teams \u00b6 Assume you have a two or more teams, each works for a different internal or external service, like: Product 1 Product 2 Observability The simplest helmfile.yaml that declares the whole cluster that is composed of the three services would look like the below: releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip - name: product2-api chart: saas-charts/api # snip - name: product2-web chart: product2-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip This works, but what if you wanted to a separate cluster per service to achieve smaller blast radius? Let\u2019s start by creating a helmfile.yaml for each service. product1/helmfile.yaml : releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip product2/helmfile.yaml : releases: - name: product2-api chart: product2-charts/api # snip - name: product2-web chart: product2-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip You will (of course!) notice this isn\u2019t DRY. To remove the duplication of observability stack between the two helmfiles, create a \u201csub-helmfile\u201d for the observability stack. observability/helmfile.yaml : - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip As you might have imagined, the observability helmfile can be reused from the two product helmfiles by declaring helmfiles . product1/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip product2/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml releases: - name: product2-api chart: product2-charts/api # snip - name: product2-web chart: product2-charts/web # snip Using sub-helmfile as a template \u00b6 You can go even further by generalizing the product related releases as a pair of api and web : shared/helmfile.yaml : releases: - name: product{{ env \"PRODUCT_ID\" }}-api chart: product{{ env \"PRODUCT_ID\" }}-charts/api # snip - name: product{{ env \"PRODUCT_ID\" }}-web chart: product{{ env \"PRODUCT_ID\" }}-charts/web # snip Then you only need one single product helmfile product/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml - ../shared/helmfile.yaml Now that we use the environment variable PRODUCT_ID to as the parameters of release names, you need to set it before running helmfile , so that it produces the differently named releases per product: $ PRODUCT_ID=1 helmfile -f product/helmfile.yaml apply $ PRODUCT_ID=2 helmfile -f product/helmfile.yaml apply","title":"Shared Configuration Across Teams"},{"location":"shared-configuration-across-teams/#shared-configuration-across-teams","text":"Assume you have a two or more teams, each works for a different internal or external service, like: Product 1 Product 2 Observability The simplest helmfile.yaml that declares the whole cluster that is composed of the three services would look like the below: releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip - name: product2-api chart: saas-charts/api # snip - name: product2-web chart: product2-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip This works, but what if you wanted to a separate cluster per service to achieve smaller blast radius? Let\u2019s start by creating a helmfile.yaml for each service. product1/helmfile.yaml : releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip product2/helmfile.yaml : releases: - name: product2-api chart: product2-charts/api # snip - name: product2-web chart: product2-charts/web # snip - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip You will (of course!) notice this isn\u2019t DRY. To remove the duplication of observability stack between the two helmfiles, create a \u201csub-helmfile\u201d for the observability stack. observability/helmfile.yaml : - name: observability-prometheus-operator chart: stable/prometheus-operator # snip - name: observability-process-exporter chart: stable/prometheus-operator # snip As you might have imagined, the observability helmfile can be reused from the two product helmfiles by declaring helmfiles . product1/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml releases: - name: product1-api chart: product1-charts/api # snip - name: product1-web chart: product1-charts/web # snip product2/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml releases: - name: product2-api chart: product2-charts/api # snip - name: product2-web chart: product2-charts/web # snip","title":"Shared Configuration Across Teams"},{"location":"shared-configuration-across-teams/#using-sub-helmfile-as-a-template","text":"You can go even further by generalizing the product related releases as a pair of api and web : shared/helmfile.yaml : releases: - name: product{{ env \"PRODUCT_ID\" }}-api chart: product{{ env \"PRODUCT_ID\" }}-charts/api # snip - name: product{{ env \"PRODUCT_ID\" }}-web chart: product{{ env \"PRODUCT_ID\" }}-charts/web # snip Then you only need one single product helmfile product/helmfile.yaml : helmfiles: - ../observability/helmfile.yaml - ../shared/helmfile.yaml Now that we use the environment variable PRODUCT_ID to as the parameters of release names, you need to set it before running helmfile , so that it produces the differently named releases per product: $ PRODUCT_ID=1 helmfile -f product/helmfile.yaml apply $ PRODUCT_ID=2 helmfile -f product/helmfile.yaml apply","title":"Using sub-helmfile as a template"},{"location":"writing-helmfile/","text":"The Helmfile Best Practices Guide \u00b6 This guide covers the Helmfile\u2019s considered patterns for writing advanced helmfiles. It focuses on how helmfile should be structured and executed. Missing keys and Default values \u00b6 helmfile tries its best to inform users for noticing potential mistakes. One example of how helmfile achieves it is that, helmfile fails when you tried to access missing keys in environment values. That is, the following example let helmfile fail when you have no eventApi.replicas defined in environment values. {{ .Values.eventApi.replicas | default 1 }} In case it isn\u2019t a mistake and you do want to allow missing keys, use the get template function: {{ .Values | get \"eventApi.replicas\" nil }} This result in printing <no value in your template, that may or may not result in a failure. If you want a kind of default values that is used when a missing key was referenced, use default like: {{ .Values | get \"eventApi.replicas\" 1 }} Now, you get 1 when there is no eventApi.replicas defined in environment values. Release Template / Conventional Directory Structure \u00b6 Introducing helmfile into a large-scale project that involves dozens of releases often results in a lot of repetitions in helmfile.yaml files. The example below shows repetitions in namespace , chart , values , and secrets : releases: # *snip* - name: heapster namespace: kube-system chart: stable/heapster version: 0.3.2 values: - \"./config/heapster/values.yaml\" - \"./config/heapster/{{ .Environment.Name }}.yaml\" secrets: - \"./config/heapster/secrets.yaml\" - \"./config/heapster/{{ .Environment.Name }}-secrets.yaml\" - name: kubernetes-dashboard namespace: kube-system chart: stable/kubernetes-dashboard version: 0.10.0 values: - \"./config/kubernetes-dashboard/values.yaml\" - \"./config/kubernetes-dashboard/{{ .Environment.Name }}.yaml\" secrets: - \"./config/kubernetes-dashboard/secrets.yaml\" - \"./config/kubernetes-dashboard/{{ .Environment.Name }}-secrets.yaml\" This is where Helmfile\u2019s advanced feature called Release Template comes handy. It allows you to abstract away the repetitions in releases into a template, which is then included and executed by using YAML anchor/alias: templates: default: &default chart: stable/{{`{{ .Release.Name }}`}} namespace: kube-system # This prevents helmfile exiting when it encounters a missing file # Valid values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\" # Use \"Debug\" to make missing files errors invisible at the default log level(--log-level=INFO) missingFileHandler: Warn values: - config/{{`{{ .Release.Name }}`}}/values.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Environment.Name }}`}}.yaml secrets: - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Environment.Name }}`}}-secrets.yaml releases: - name: heapster <<: *default - name: kubernetes-dashboard <<: *default Release Templating supports the following parts of release definition: - basic fields: name , namespace , chart , version - boolean fields: installed , wait , tillerless , verify by the means of additional text fields designed for templating only: installedTemplate , waitTemplate , tillerlessTemplate , verifyTemplate yaml # ... installedTemplate: '{{`{{ eq .Release.Namespace \"kube-system\" }}`}}' waitTemplate: '{{`{{ eq .Release.Labels.tag \"safe\" | not }}`}}' # ... - set block values: yaml # ... setTemplate: - name: '{{`{{ .Release.Name }}`}}' values: '{{`{{ .Release.Namespace }}`}}' # ... - values and secrets file paths: yaml # ... valuesTemplate: - config/{{`{{ .Release.Name }}`}}/values.yaml secrets: - config/{{`{{ .Release.Name }}`}}/secrets.yaml # ... - inline values map: yaml # ... valuesTemplate: - image: tag: `{{ .Release.Labels.tag }}` # ... See the issue 428 for more context on how this is supposed to work. Layering Release Values \u00b6 Please note, that it is not possible to layer values sections. If values is defined in the release and in the release template, only the values defined in the release will be considered. The same applies to secrets and set . Layering State Files \u00b6 See Layering State Template Files if you\u2019re layering templates. You may occasionally end up with many helmfiles that shares common parts like which repositories to use, and which release to be bundled by default. Use Layering to extract the common parts into a dedicated library helmfile s, so that each helmfile becomes DRY. Let\u2019s assume that your helmfile.yaml looks like: bases: - environments.yaml releases: - name: metricbeat chart: stable/metricbeat - name: myapp chart: mychart Whereas environments.yaml contained well-known environments: environments: development: production: At run time, bases in your helmfile.yaml are evaluated to produce: --- # environments.yaml environments: development: production: --- # helmfile.yaml releases: - name: myapp chart: mychart - name: metricbeat chart: stable/metricbeat Finally the resulting YAML documents are merged in the order of occurrence, so that your helmfile.yaml becomes: environments: development: production: releases: - name: metricbeat chart: stable/metricbeat - name: myapp chart: mychart Great! Now, repeat the above steps for each your helmfile.yaml , so that all your helmfiles becomes DRY. Please also see the discussion in the issue 388 for more advanced layering examples. Merging Arrays in Layers \u00b6 Helmfile doesn\u2019t merge arrays across layers. That is, the below example doesn\u2019t work as you might have expected: releases: - name: metricbeat chart: stable/metricbeat --- releases: - name: myapp chart: mychart Helmfile overrides the releases array with the latest layer so the resulting state file will be: releases: # metricbeat release disappeared! but that's how helmfile works - name: myapp chart: mychart A work-around is to treat the state file as a go template and use readFile template function to import the common part of your state file as a plain text: common.yaml : templates: metricbeat: &metricbeat name: metricbeat chart: stable/metricbeat helmfile.yaml : {{ readFile \"common.yaml\" }} releases: - <<: *metricbeat - name: myapp chart: mychart Layering State Template Files \u00b6 Do you need to make your state file even more DRY? Turned out layering state files wasn\u2019t enough for you? Helmfile supports an advanced feature that allows you to compose state \u201ctemplate\u201d files to generate the final state to be processed. In the following example helmfile.yaml.gotmpl , each --- separated part of the file is a go template. helmfile.yaml.gotmpl : # Part 1: Reused Enviroment Values bases: - myenv.yaml --- # Part 2: Reused Defaults bases: - mydefaults.yaml --- # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-{{ .Values.myname }} values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\" Suppose the myenv.yaml and test.env.yaml loaded in the first part looks like: myenv.yaml : environments: test: values: - test.env.yaml test.env.yaml : kubeContext: test wait: false cvOnly: false myname: \"dog\" Where the gotmpl file loaded in the second part looks like: mydefaults.yaml.gotmpl : helmDefaults: tillerNamespace: kube-system kubeContext: {{ .Values.kubeContext }} verify: false {{ if .Values.wait }} wait: true {{ else }} wait: false {{ end }} timeout: 600 recreatePods: false force: true Each go template is rendered in the context where .Values is inherited from the previous part. So in mydefaults.yaml.gotmpl , both .Values.kubeContext and .Values.wait are valid as they do exist in the environment values inherited from the previous part(=the first part) of your helmfile.yaml.gotmpl , and therefore the template is rendered to: helmDefaults: tillerNamespace: kube-system kubeContext: test verify: false wait: false timeout: 600 recreatePods: false force: true Similarly, the third part of the top-level helmfile.yaml.gotmpl , .Values.myname is valid as it is included in the environment values inherited from the previous parts: # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-{{ .Values.myname }} values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\" ```` hence rendered to: ```yaml # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-dog values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\"","title":"Best Practices Guide"},{"location":"writing-helmfile/#the-helmfile-best-practices-guide","text":"This guide covers the Helmfile\u2019s considered patterns for writing advanced helmfiles. It focuses on how helmfile should be structured and executed.","title":"The Helmfile Best Practices Guide"},{"location":"writing-helmfile/#missing-keys-and-default-values","text":"helmfile tries its best to inform users for noticing potential mistakes. One example of how helmfile achieves it is that, helmfile fails when you tried to access missing keys in environment values. That is, the following example let helmfile fail when you have no eventApi.replicas defined in environment values. {{ .Values.eventApi.replicas | default 1 }} In case it isn\u2019t a mistake and you do want to allow missing keys, use the get template function: {{ .Values | get \"eventApi.replicas\" nil }} This result in printing <no value in your template, that may or may not result in a failure. If you want a kind of default values that is used when a missing key was referenced, use default like: {{ .Values | get \"eventApi.replicas\" 1 }} Now, you get 1 when there is no eventApi.replicas defined in environment values.","title":"Missing keys and Default values"},{"location":"writing-helmfile/#release-template-conventional-directory-structure","text":"Introducing helmfile into a large-scale project that involves dozens of releases often results in a lot of repetitions in helmfile.yaml files. The example below shows repetitions in namespace , chart , values , and secrets : releases: # *snip* - name: heapster namespace: kube-system chart: stable/heapster version: 0.3.2 values: - \"./config/heapster/values.yaml\" - \"./config/heapster/{{ .Environment.Name }}.yaml\" secrets: - \"./config/heapster/secrets.yaml\" - \"./config/heapster/{{ .Environment.Name }}-secrets.yaml\" - name: kubernetes-dashboard namespace: kube-system chart: stable/kubernetes-dashboard version: 0.10.0 values: - \"./config/kubernetes-dashboard/values.yaml\" - \"./config/kubernetes-dashboard/{{ .Environment.Name }}.yaml\" secrets: - \"./config/kubernetes-dashboard/secrets.yaml\" - \"./config/kubernetes-dashboard/{{ .Environment.Name }}-secrets.yaml\" This is where Helmfile\u2019s advanced feature called Release Template comes handy. It allows you to abstract away the repetitions in releases into a template, which is then included and executed by using YAML anchor/alias: templates: default: &default chart: stable/{{`{{ .Release.Name }}`}} namespace: kube-system # This prevents helmfile exiting when it encounters a missing file # Valid values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\" # Use \"Debug\" to make missing files errors invisible at the default log level(--log-level=INFO) missingFileHandler: Warn values: - config/{{`{{ .Release.Name }}`}}/values.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Environment.Name }}`}}.yaml secrets: - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Environment.Name }}`}}-secrets.yaml releases: - name: heapster <<: *default - name: kubernetes-dashboard <<: *default Release Templating supports the following parts of release definition: - basic fields: name , namespace , chart , version - boolean fields: installed , wait , tillerless , verify by the means of additional text fields designed for templating only: installedTemplate , waitTemplate , tillerlessTemplate , verifyTemplate yaml # ... installedTemplate: '{{`{{ eq .Release.Namespace \"kube-system\" }}`}}' waitTemplate: '{{`{{ eq .Release.Labels.tag \"safe\" | not }}`}}' # ... - set block values: yaml # ... setTemplate: - name: '{{`{{ .Release.Name }}`}}' values: '{{`{{ .Release.Namespace }}`}}' # ... - values and secrets file paths: yaml # ... valuesTemplate: - config/{{`{{ .Release.Name }}`}}/values.yaml secrets: - config/{{`{{ .Release.Name }}`}}/secrets.yaml # ... - inline values map: yaml # ... valuesTemplate: - image: tag: `{{ .Release.Labels.tag }}` # ... See the issue 428 for more context on how this is supposed to work.","title":"Release Template / Conventional Directory Structure"},{"location":"writing-helmfile/#layering-release-values","text":"Please note, that it is not possible to layer values sections. If values is defined in the release and in the release template, only the values defined in the release will be considered. The same applies to secrets and set .","title":"Layering Release Values"},{"location":"writing-helmfile/#layering-state-files","text":"See Layering State Template Files if you\u2019re layering templates. You may occasionally end up with many helmfiles that shares common parts like which repositories to use, and which release to be bundled by default. Use Layering to extract the common parts into a dedicated library helmfile s, so that each helmfile becomes DRY. Let\u2019s assume that your helmfile.yaml looks like: bases: - environments.yaml releases: - name: metricbeat chart: stable/metricbeat - name: myapp chart: mychart Whereas environments.yaml contained well-known environments: environments: development: production: At run time, bases in your helmfile.yaml are evaluated to produce: --- # environments.yaml environments: development: production: --- # helmfile.yaml releases: - name: myapp chart: mychart - name: metricbeat chart: stable/metricbeat Finally the resulting YAML documents are merged in the order of occurrence, so that your helmfile.yaml becomes: environments: development: production: releases: - name: metricbeat chart: stable/metricbeat - name: myapp chart: mychart Great! Now, repeat the above steps for each your helmfile.yaml , so that all your helmfiles becomes DRY. Please also see the discussion in the issue 388 for more advanced layering examples.","title":"Layering State Files"},{"location":"writing-helmfile/#merging-arrays-in-layers","text":"Helmfile doesn\u2019t merge arrays across layers. That is, the below example doesn\u2019t work as you might have expected: releases: - name: metricbeat chart: stable/metricbeat --- releases: - name: myapp chart: mychart Helmfile overrides the releases array with the latest layer so the resulting state file will be: releases: # metricbeat release disappeared! but that's how helmfile works - name: myapp chart: mychart A work-around is to treat the state file as a go template and use readFile template function to import the common part of your state file as a plain text: common.yaml : templates: metricbeat: &metricbeat name: metricbeat chart: stable/metricbeat helmfile.yaml : {{ readFile \"common.yaml\" }} releases: - <<: *metricbeat - name: myapp chart: mychart","title":"Merging Arrays in Layers"},{"location":"writing-helmfile/#layering-state-template-files","text":"Do you need to make your state file even more DRY? Turned out layering state files wasn\u2019t enough for you? Helmfile supports an advanced feature that allows you to compose state \u201ctemplate\u201d files to generate the final state to be processed. In the following example helmfile.yaml.gotmpl , each --- separated part of the file is a go template. helmfile.yaml.gotmpl : # Part 1: Reused Enviroment Values bases: - myenv.yaml --- # Part 2: Reused Defaults bases: - mydefaults.yaml --- # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-{{ .Values.myname }} values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\" Suppose the myenv.yaml and test.env.yaml loaded in the first part looks like: myenv.yaml : environments: test: values: - test.env.yaml test.env.yaml : kubeContext: test wait: false cvOnly: false myname: \"dog\" Where the gotmpl file loaded in the second part looks like: mydefaults.yaml.gotmpl : helmDefaults: tillerNamespace: kube-system kubeContext: {{ .Values.kubeContext }} verify: false {{ if .Values.wait }} wait: true {{ else }} wait: false {{ end }} timeout: 600 recreatePods: false force: true Each go template is rendered in the context where .Values is inherited from the previous part. So in mydefaults.yaml.gotmpl , both .Values.kubeContext and .Values.wait are valid as they do exist in the environment values inherited from the previous part(=the first part) of your helmfile.yaml.gotmpl , and therefore the template is rendered to: helmDefaults: tillerNamespace: kube-system kubeContext: test verify: false wait: false timeout: 600 recreatePods: false force: true Similarly, the third part of the top-level helmfile.yaml.gotmpl , .Values.myname is valid as it is included in the environment values inherited from the previous parts: # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-{{ .Values.myname }} values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\" ```` hence rendered to: ```yaml # Part 3: Dynamic Releases releases: - name: test1 chart: mychart-dog values: replicaCount: 1 image: repository: \"nginx\" tag: \"latest\"","title":"Layering State Template Files"}]}